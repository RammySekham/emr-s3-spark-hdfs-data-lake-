{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"elt\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data1 = \"data/log-data/*.json\"\n",
    "input_data2 = \"data/song_data/*/*/*/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json(input_data1, multiLine=True)\n",
    "dfs = spark.read.json(input_data2, multiLine=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NextSong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page\n",
       "0      Home\n",
       "1     Login\n",
       "2  NextSong"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"page\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "dff = df.filter((col(\"page\") =='NextSong') & col(\"userId\").isNotNull()).groupby('userId').agg({'ts':'max'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- max(ts): long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## user table\n",
    "dfl=df.join(dff, (df.ts==col(\"max(ts)\")) & (df.userId==dff.userId), 'right').select(df.userId.alias(\"user_id\"), col(\"firstName\").alias(\"first_name\"), \n",
    "                                             col(\"lastName\").alias(\"last_name\"), \"gender\", \"level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Getting normal columns from table \n",
    "dft=df.filter(\"page='NextSong'\")\n",
    "dft.printSchema()\n",
    "dft.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(tsm='2018-11-23 11:07:25'),\n",
       " Row(tsm='2018-11-20 11:00:42'),\n",
       " Row(tsm='2018-11-24 11:45:00'),\n",
       " Row(tsm='2018-11-29 11:00:57'),\n",
       " Row(tsm='2018-11-05 11:33:12'),\n",
       " Row(tsm='2018-11-16 11:00:57'),\n",
       " Row(tsm='2018-11-17 11:02:24'),\n",
       " Row(tsm='2018-11-12 13:36:57'),\n",
       " Row(tsm='2018-11-25 12:39:14'),\n",
       " Row(tsm='2018-11-07 11:01:16'),\n",
       " Row(tsm='2018-11-26 11:02:43'),\n",
       " Row(tsm='2018-11-04 11:15:55'),\n",
       " Row(tsm='2018-11-02 12:25:34'),\n",
       " Row(tsm='2018-11-11 13:33:56'),\n",
       " Row(tsm='2018-11-21 11:27:34'),\n",
       " Row(tsm='2018-11-27 11:52:12'),\n",
       " Row(tsm='2018-11-19 12:54:28'),\n",
       " Row(tsm='2018-11-06 13:12:44'),\n",
       " Row(tsm='2018-11-15 11:30:26'),\n",
       " Row(tsm='2018-11-02 07:57:10'),\n",
       " Row(tsm='2018-11-22 11:03:52'),\n",
       " Row(tsm='2018-11-10 11:15:27'),\n",
       " Row(tsm='2018-11-28 11:00:15'),\n",
       " Row(tsm='2018-11-14 11:03:22'),\n",
       " Row(tsm='2018-11-30 11:22:07'),\n",
       " Row(tsm='2018-11-09 11:06:17'),\n",
       " Row(tsm='2018-11-13 11:36:57'),\n",
       " Row(tsm='2018-11-03 12:04:33'),\n",
       " Row(tsm='2018-11-08 11:12:30')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Solving Timestamp problem\n",
    "# get_timestamp = udf()\n",
    "date_time = df.select(from_unixtime(col(\"ts\")/1000).alias('tsm'))\n",
    "df_test = date_time.dropDuplicates()\n",
    "df_test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## songplays table\n",
    "new_df = dft.join(dfs,(dft.artist==dfs.artist_name) & (dft.song==dfs.title), 'left').select(from_unixtime(col(\"ts\")/1000).alias(\"start_time\"), col(\"userId\").alias(\"user_id\"),\"level\", dfs.song_id, dfs.artist_id, col(\"sessionId\").alias(\"session_id\"), \"location\",col(\"userAgent\").alias(\"user_agent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "window= Window.orderBy(\"start_time\")\n",
    "dfg = new_df.withColumn('songplay_id', row_number().over(window))\n",
    "dfg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## time table\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "#start_time, hour, day, week, month, year, weekday\n",
    "time_table = date_time.select(col(\"tsm\").alias(\"start_time\"), hour(col(\"tsm\")).alias(\"hour\"), dayofmonth(col(\"tsm\")).alias(\"day\"), weekofyear(col(\"tsm\")).alias(\"week\"), month(col(\"tsm\")).alias(\"month\"), year(col(\"tsm\")).alias(\"year\"), dayofweek(col(\"tsm\")).alias(\"weekday\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|         start_time|hour|day|week|month|year|weekday|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-15 11:30:26|  11| 15|  46|   11|2018|      5|\n",
      "|2018-11-21 11:27:34|  11| 21|  47|   11|2018|      4|\n",
      "|2018-11-14 11:03:22|  11| 14|  46|   11|2018|      4|\n",
      "|2018-11-28 11:00:15|  11| 28|  48|   11|2018|      4|\n",
      "|2018-11-05 11:33:12|  11|  5|  45|   11|2018|      2|\n",
      "|2018-11-13 11:36:57|  11| 13|  46|   11|2018|      3|\n",
      "|2018-11-30 11:22:07|  11| 30|  48|   11|2018|      6|\n",
      "|2018-11-16 11:00:57|  11| 16|  46|   11|2018|      6|\n",
      "|2018-11-20 11:00:42|  11| 20|  47|   11|2018|      3|\n",
      "|2018-11-24 11:45:00|  11| 24|  47|   11|2018|      7|\n",
      "|2018-11-29 11:00:57|  11| 29|  48|   11|2018|      5|\n",
      "|2018-11-19 12:54:28|  12| 19|  47|   11|2018|      2|\n",
      "|2018-11-27 11:52:12|  11| 27|  48|   11|2018|      3|\n",
      "|2018-11-23 11:07:25|  11| 23|  47|   11|2018|      6|\n",
      "|2018-11-09 11:06:17|  11|  9|  45|   11|2018|      6|\n",
      "|2018-11-26 11:02:43|  11| 26|  48|   11|2018|      2|\n",
      "|2018-11-08 11:12:30|  11|  8|  45|   11|2018|      5|\n",
      "|2018-11-12 13:36:57|  13| 12|  46|   11|2018|      2|\n",
      "|2018-11-07 11:01:16|  11|  7|  45|   11|2018|      4|\n",
      "|2018-11-04 11:15:55|  11|  4|  44|   11|2018|      1|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Artist Table\n",
    "#artist_id, name, location, lattitude, longitude\n",
    "artist = dfs.filter(\"artist_id is NOT NULL\").select(\"artist_id\", col(\"artist_location\").alias(\"location\"), col(\"artist_latitude\").alias(\"latitude\"),\\\n",
    "           col(\"artist_longitude\").alias(\"longitude\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = dfs.alias(\"one\").filter(\"artist_id is NOT NULL\").groupby(\"artist_id\").agg({'year':'max'}).orderBy(\"artist_id\")\n",
    "a1.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## logic to get artist' latest details by year, \n",
    "## when artist has two updates per year, records are taken as sort order whichever appear first\n",
    "\n",
    "a2 = dfs.alias(\"one\").filter(\"artist_id is NOT NULL\").groupby(\"artist_id\").agg({'year':'max'})\\\n",
    ".join(dfs.alias(\"two\"), (col(\"one.artist_id\")==col(\"two.artist_id\")) & (\"max(year)\"==col(\"two.year\")), 'left')\\\n",
    ".select(\"one.artist_id\", col(\"artist_location\").alias(\"location\"), col(\"artist_latitude\").alias(\"latitude\"),\\\n",
    "           col(\"artist_longitude\").alias(\"longitude\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let us optimize query if possible\n",
    "a2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Song Table\n",
    "dfs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# song_id, title, artist_id, year, duration\n",
    "song = dfs.filter(\"song_id is NOT NULL\")\\\n",
    ".select(\"song_id\",\"title\", \"artist_id\", \"year\", \"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song.select(\"song_id\", \"artist_id\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "song.write.parquet(\"Data/song\", mode=\"overwrite\", partitionBy=('year', 'artist_id'), compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parquet in module pyspark.sql.readwriter:\n",
      "\n",
      "parquet(path, mode=None, partitionBy=None, compression=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      "    \n",
      "    :param path: the path in any Hadoop supported file system\n",
      "    :param mode: specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "    :param partitionBy: names of partitioning columns\n",
      "    :param compression: compression codec to use when saving to file. This can be one of the\n",
      "                        known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n",
      "                        lzo, brotli, lz4, and zstd). This will override\n",
      "                        ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n",
      "                        value specified in ``spark.sql.parquet.compression.codec``.\n",
      "    \n",
      "    >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.write.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
